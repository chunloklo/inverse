{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loadFilteredData import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Chunlok Lo\\\\Documents\\\\cs4476\\\\Inverse\\\\data\\\\a\\\\abbey'\n",
    "filPath = 'C:\\\\Users\\\\Chunlok Lo\\\\Documents\\\\cs4476\\\\Inverse\\\\data\\\\a_clarendon\\\\abbey'\n",
    "\n",
    "origImg, filImg = loadFilteredData(path, filPath)\n",
    "\n",
    "X, y, Xtest, ytest = createData([origImg, filImg], 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "scores = cross_val_score(clf, X.reshape(X.shape[0], -1), y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.625     , 0.625     , 0.64166667, 0.63333333, 0.61666667])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X.reshape(X.shape[0], -1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "predict = clf.predict(Xtest.reshape(Xtest.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def modelMetric(ytest, predict):\n",
    "    print(accuracy_score(ytest, predict))\n",
    "    print(confusion_matrix(ytest.argmax(axis=1), predict.argmax(axis=1), [0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogramPreprocessing(images, bins=10000):\n",
    "    histograms = []\n",
    "    for image in images:\n",
    "        histograms.append(np.histogram(image, bins=bins, range=(0, 255))[0])\n",
    "    \n",
    "    return np.stack(histograms)\n",
    "\n",
    "def flat(images):\n",
    "    return images.reshape(images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xhist = histogramPreprocessing(X)\n",
    "clf.fit(Xhist, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "[[50 50]\n",
      " [30 70]]\n"
     ]
    }
   ],
   "source": [
    "predict = clf.predict(histogramPreprocessing(Xtest))\n",
    "modelMetric(ytest, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chunlok Lo\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "[[74 26]\n",
      " [34 66]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "clf.fit(histogramPreprocessing(X), y.argmax(axis=1))\n",
    "predict = clf.predict(histogramPreprocessing(Xtest))\n",
    "print(accuracy_score(ytest.argmax(axis=1), predict))\n",
    "print(confusion_matrix(ytest.argmax(axis=1), predict, [0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.605\n",
      "[[63 37]\n",
      " [42 58]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chunlok Lo\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "clf.fit(flat(X), y.argmax(axis=1))\n",
    "predict = clf.predict(flat(Xtest))\n",
    "print(accuracy_score(ytest.argmax(axis=1), predict))\n",
    "print(confusion_matrix(ytest.argmax(axis=1), predict, [0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 10.68991756\n",
      "Iteration 2, loss = 4.03883867\n",
      "Iteration 3, loss = 2.85288890\n",
      "Iteration 4, loss = 2.28455585\n",
      "Iteration 5, loss = 1.86087413\n",
      "Iteration 6, loss = 1.73751906\n",
      "Iteration 7, loss = 1.47639138\n",
      "Iteration 8, loss = 1.48156056\n",
      "Iteration 9, loss = 1.41141899\n",
      "Iteration 10, loss = 1.37037049\n",
      "Iteration 11, loss = 1.40502707\n",
      "Iteration 12, loss = 1.30192063\n",
      "Iteration 13, loss = 1.23829380\n",
      "Iteration 14, loss = 1.21249953\n",
      "Iteration 15, loss = 1.14302919\n",
      "Iteration 16, loss = 1.21423487\n",
      "Iteration 17, loss = 1.13256238\n",
      "Iteration 18, loss = 1.12979901\n",
      "Iteration 19, loss = 1.14656195\n",
      "Iteration 20, loss = 1.09967422\n",
      "Iteration 21, loss = 1.23247445\n",
      "Iteration 22, loss = 1.28246066\n",
      "Iteration 23, loss = 1.43194293\n",
      "Iteration 24, loss = 1.42544680\n",
      "Iteration 25, loss = 1.36839756\n",
      "Iteration 26, loss = 1.21487036\n",
      "Iteration 27, loss = 1.09260226\n",
      "Iteration 28, loss = 1.08751173\n",
      "Iteration 29, loss = 1.02356330\n",
      "Iteration 30, loss = 1.02349215\n",
      "Iteration 31, loss = 1.06318837\n",
      "Iteration 32, loss = 0.97605082\n",
      "Iteration 33, loss = 0.93313364\n",
      "Iteration 34, loss = 0.92793943\n",
      "Iteration 35, loss = 0.91000959\n",
      "Iteration 36, loss = 0.86970354\n",
      "Iteration 37, loss = 0.89905535\n",
      "Iteration 38, loss = 0.89914653\n",
      "Iteration 39, loss = 0.88242565\n",
      "Iteration 40, loss = 0.85896908\n",
      "Iteration 41, loss = 0.84791972\n",
      "Iteration 42, loss = 0.85694023\n",
      "Iteration 43, loss = 0.86706564\n",
      "Iteration 44, loss = 0.97235252\n",
      "Iteration 45, loss = 0.84035352\n",
      "Iteration 46, loss = 0.86125697\n",
      "Iteration 47, loss = 0.86560139\n",
      "Iteration 48, loss = 0.87934634\n",
      "Iteration 49, loss = 0.78809185\n",
      "Iteration 50, loss = 0.83730402\n",
      "Iteration 51, loss = 0.75946939\n",
      "Iteration 52, loss = 0.88836944\n",
      "Iteration 53, loss = 0.82943359\n",
      "Iteration 54, loss = 0.84599736\n",
      "Iteration 55, loss = 0.76608575\n",
      "Iteration 56, loss = 0.80413197\n",
      "Iteration 57, loss = 0.80800572\n",
      "Iteration 58, loss = 0.87758804\n",
      "Iteration 59, loss = 0.85241505\n",
      "Iteration 60, loss = 0.75084967\n",
      "Iteration 61, loss = 0.73085650\n",
      "Iteration 62, loss = 0.75975550\n",
      "Iteration 63, loss = 0.87713447\n",
      "Iteration 64, loss = 0.87292287\n",
      "Iteration 65, loss = 0.84759855\n",
      "Iteration 66, loss = 0.81402427\n",
      "Iteration 67, loss = 0.78394935\n",
      "Iteration 68, loss = 0.71918478\n",
      "Iteration 69, loss = 0.69497828\n",
      "Iteration 70, loss = 0.73616500\n",
      "Iteration 71, loss = 0.69544974\n",
      "Iteration 72, loss = 0.63758145\n",
      "Iteration 73, loss = 0.63958938\n",
      "Iteration 74, loss = 0.59746576\n",
      "Iteration 75, loss = 0.64958036\n",
      "Iteration 76, loss = 0.65486809\n",
      "Iteration 77, loss = 0.64761560\n",
      "Iteration 78, loss = 0.67058833\n",
      "Iteration 79, loss = 0.89295712\n",
      "Iteration 80, loss = 0.70670790\n",
      "Iteration 81, loss = 0.66841185\n",
      "Iteration 82, loss = 0.69212976\n",
      "Iteration 83, loss = 0.79820972\n",
      "Iteration 84, loss = 0.75466167\n",
      "Iteration 85, loss = 0.61263447\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fd6000ad3797>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistogramPreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistogramPreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, verbose=10, tol=0.000000001)\n",
    "clf.fit(histogramPreprocessing(X), y)\n",
    "predict = clf.predict(histogramPreprocessing(Xtest))\n",
    "print(accuracy_score(ytest.argmax(axis=1), predict))\n",
    "print(confusion_matrix(ytest.argmax(axis=1), predict, [0, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8511111111111112"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = clf.predict(histogramPreprocessing(Xtest))\n",
    "print(accuracy_score(ytest, predict))\n",
    "clf.score(histogramPreprocessing(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbey', 'airport_terminal', 'amphitheater', 'amusement_park', 'aquarium', 'aqueduct', 'art_gallery', 'assembly_line', 'auditorium']\n",
      "Loading Topic1: abbey\n",
      "Loading Topic1: airport_terminal\n",
      "Loading Topic1: amphitheater\n",
      "Loading Topic1: amusement_park\n",
      "Loading Topic1: aquarium\n",
      "Loading Topic1: aqueduct\n",
      "Loading Topic1: art_gallery\n",
      "Loading Topic1: assembly_line\n",
      "Loading Topic1: auditorium\n",
      "(9000, 128, 128, 3)\n",
      "(9000, 128, 128, 3)\n",
      "9000\n",
      "9000\n",
      "[8100. 8100.]\n",
      "[900. 900.]\n"
     ]
    }
   ],
   "source": [
    "from loadFilteredData import *\n",
    "#loading all data\n",
    "origImg, filImg = loadAllTopicData()\n",
    "X, y, Xtest, ytest = createData([origImg, filImg], .9)\n",
    "print(np.sum(y, axis=0))\n",
    "print(np.sum(ytest, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=10000, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "16200/16200 [==============================] - 4s 273us/step - loss: 2.0383 - acc: 0.5859\n",
      "Epoch 2/150\n",
      "16200/16200 [==============================] - 3s 198us/step - loss: 0.7111 - acc: 0.6264\n",
      "Epoch 3/150\n",
      "16200/16200 [==============================] - 3s 186us/step - loss: 0.6693 - acc: 0.6499\n",
      "Epoch 4/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.6459 - acc: 0.6610\n",
      "Epoch 5/150\n",
      "16200/16200 [==============================] - 3s 177us/step - loss: 0.6333 - acc: 0.6660\n",
      "Epoch 6/150\n",
      "16200/16200 [==============================] - 3s 176us/step - loss: 0.6126 - acc: 0.6787\n",
      "Epoch 7/150\n",
      "16200/16200 [==============================] - 3s 175us/step - loss: 0.6050 - acc: 0.6835\n",
      "Epoch 8/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.6090 - acc: 0.6835\n",
      "Epoch 9/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5910 - acc: 0.6983\n",
      "Epoch 10/150\n",
      "16200/16200 [==============================] - 3s 177us/step - loss: 0.5930 - acc: 0.6952\n",
      "Epoch 11/150\n",
      "16200/16200 [==============================] - 3s 177us/step - loss: 0.5848 - acc: 0.7038\n",
      "Epoch 12/150\n",
      "16200/16200 [==============================] - 3s 198us/step - loss: 0.6028 - acc: 0.6822\n",
      "Epoch 13/150\n",
      "16200/16200 [==============================] - 3s 199us/step - loss: 0.5983 - acc: 0.6896\n",
      "Epoch 14/150\n",
      "16200/16200 [==============================] - 3s 198us/step - loss: 0.5810 - acc: 0.7046\n",
      "Epoch 15/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5846 - acc: 0.7015\n",
      "Epoch 16/150\n",
      "16200/16200 [==============================] - 3s 193us/step - loss: 0.5826 - acc: 0.6999\n",
      "Epoch 17/150\n",
      "16200/16200 [==============================] - 3s 188us/step - loss: 0.5783 - acc: 0.7072\n",
      "Epoch 18/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.5700 - acc: 0.7128\n",
      "Epoch 19/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.5730 - acc: 0.7091\n",
      "Epoch 20/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.5708 - acc: 0.7148\n",
      "Epoch 21/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.5659 - acc: 0.7207\n",
      "Epoch 22/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5700 - acc: 0.7133\n",
      "Epoch 23/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5786 - acc: 0.7038\n",
      "Epoch 24/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5569 - acc: 0.7294\n",
      "Epoch 25/150\n",
      "16200/16200 [==============================] - 3s 188us/step - loss: 0.5633 - acc: 0.7168\n",
      "Epoch 26/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.5653 - acc: 0.7155\n",
      "Epoch 27/150\n",
      "16200/16200 [==============================] - 3s 185us/step - loss: 0.5661 - acc: 0.7159\n",
      "Epoch 28/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.5674 - acc: 0.7162\n",
      "Epoch 29/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.5516 - acc: 0.7253\n",
      "Epoch 30/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5527 - acc: 0.7248\n",
      "Epoch 31/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.5496 - acc: 0.7285\n",
      "Epoch 32/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5533 - acc: 0.7256\n",
      "Epoch 33/150\n",
      "16200/16200 [==============================] - 3s 185us/step - loss: 0.5449 - acc: 0.7340\n",
      "Epoch 34/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5440 - acc: 0.7291\n",
      "Epoch 35/150\n",
      "16200/16200 [==============================] - 3s 195us/step - loss: 0.5419 - acc: 0.7299\n",
      "Epoch 36/150\n",
      "16200/16200 [==============================] - 3s 191us/step - loss: 0.5470 - acc: 0.7259\n",
      "Epoch 37/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5355 - acc: 0.7322\n",
      "Epoch 38/150\n",
      "16200/16200 [==============================] - 3s 186us/step - loss: 0.5497 - acc: 0.7261\n",
      "Epoch 39/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.5261 - acc: 0.7465\n",
      "Epoch 40/150\n",
      "16200/16200 [==============================] - 3s 177us/step - loss: 0.5363 - acc: 0.7373\n",
      "Epoch 41/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.5319 - acc: 0.7384\n",
      "Epoch 42/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.5402 - acc: 0.7325\n",
      "Epoch 43/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.5342 - acc: 0.7334\n",
      "Epoch 44/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.5265 - acc: 0.7423\n",
      "Epoch 45/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.5246 - acc: 0.7394\n",
      "Epoch 46/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5316 - acc: 0.7420\n",
      "Epoch 47/150\n",
      "16200/16200 [==============================] - 3s 186us/step - loss: 0.5349 - acc: 0.7348\n",
      "Epoch 48/150\n",
      "16200/16200 [==============================] - 3s 205us/step - loss: 0.5253 - acc: 0.7417\n",
      "Epoch 49/150\n",
      "16200/16200 [==============================] - 3s 201us/step - loss: 0.5270 - acc: 0.7410 0s - loss: 0.5273 - acc: 0\n",
      "Epoch 50/150\n",
      "16200/16200 [==============================] - 3s 195us/step - loss: 0.5192 - acc: 0.7459 1s - loss: 0.\n",
      "Epoch 51/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.5304 - acc: 0.7368\n",
      "Epoch 52/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5332 - acc: 0.7346\n",
      "Epoch 53/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.5244 - acc: 0.7432\n",
      "Epoch 54/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5331 - acc: 0.7356\n",
      "Epoch 55/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5217 - acc: 0.7425\n",
      "Epoch 56/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.5250 - acc: 0.7419\n",
      "Epoch 57/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.5215 - acc: 0.7402\n",
      "Epoch 58/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5186 - acc: 0.7480 0s - loss: 0.5170 - a\n",
      "Epoch 59/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5314 - acc: 0.7338\n",
      "Epoch 60/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.5260 - acc: 0.7447\n",
      "Epoch 61/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.5181 - acc: 0.7454\n",
      "Epoch 62/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5454 - acc: 0.7227\n",
      "Epoch 63/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5264 - acc: 0.7407\n",
      "Epoch 64/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5211 - acc: 0.7456\n",
      "Epoch 65/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5263 - acc: 0.7410\n",
      "Epoch 66/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5264 - acc: 0.7399\n",
      "Epoch 67/150\n",
      "16200/16200 [==============================] - 3s 185us/step - loss: 0.5294 - acc: 0.7367\n",
      "Epoch 68/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5157 - acc: 0.7476\n",
      "Epoch 69/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5168 - acc: 0.7472\n",
      "Epoch 70/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.5165 - acc: 0.7475\n",
      "Epoch 71/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5100 - acc: 0.7530\n",
      "Epoch 72/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.5182 - acc: 0.7483\n",
      "Epoch 73/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5070 - acc: 0.7536\n",
      "Epoch 74/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5091 - acc: 0.7512\n",
      "Epoch 75/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.5269 - acc: 0.7418\n",
      "Epoch 76/150\n",
      "16200/16200 [==============================] - 3s 177us/step - loss: 0.5186 - acc: 0.7467\n",
      "Epoch 77/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5150 - acc: 0.7459\n",
      "Epoch 78/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.5089 - acc: 0.7538\n",
      "Epoch 79/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.5132 - acc: 0.7503\n",
      "Epoch 80/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.5126 - acc: 0.7523\n",
      "Epoch 81/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5128 - acc: 0.7491\n",
      "Epoch 82/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.5115 - acc: 0.7499\n",
      "Epoch 83/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.5084 - acc: 0.7506\n",
      "Epoch 84/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5115 - acc: 0.7499\n",
      "Epoch 85/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.5071 - acc: 0.7562\n",
      "Epoch 86/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.5151 - acc: 0.7445\n",
      "Epoch 87/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.5104 - acc: 0.7529\n",
      "Epoch 88/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.5110 - acc: 0.7531\n",
      "Epoch 89/150\n",
      "16200/16200 [==============================] - 3s 177us/step - loss: 0.5038 - acc: 0.7580\n",
      "Epoch 90/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.5005 - acc: 0.7581\n",
      "Epoch 91/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5160 - acc: 0.7468\n",
      "Epoch 92/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.5082 - acc: 0.7531\n",
      "Epoch 93/150\n",
      "16200/16200 [==============================] - 3s 177us/step - loss: 0.5210 - acc: 0.7427\n",
      "Epoch 94/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5089 - acc: 0.7501\n",
      "Epoch 95/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.4992 - acc: 0.7616\n",
      "Epoch 96/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.5040 - acc: 0.7558\n",
      "Epoch 97/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5050 - acc: 0.7556\n",
      "Epoch 98/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.5084 - acc: 0.7542\n",
      "Epoch 99/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5023 - acc: 0.7553\n",
      "Epoch 100/150\n",
      "16200/16200 [==============================] - 3s 177us/step - loss: 0.5020 - acc: 0.7551\n",
      "Epoch 101/150\n",
      "16200/16200 [==============================] - 3s 177us/step - loss: 0.4994 - acc: 0.7604 1s - l\n",
      "Epoch 102/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5126 - acc: 0.7474\n",
      "Epoch 103/150\n",
      "16200/16200 [==============================] - 3s 176us/step - loss: 0.5003 - acc: 0.7575\n",
      "Epoch 104/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.5021 - acc: 0.7580\n",
      "Epoch 105/150\n",
      "16200/16200 [==============================] - 3s 176us/step - loss: 0.5052 - acc: 0.7551 \n",
      "Epoch 106/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.4976 - acc: 0.7618\n",
      "Epoch 107/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5205 - acc: 0.7452\n",
      "Epoch 108/150\n",
      "16200/16200 [==============================] - 3s 187us/step - loss: 0.5100 - acc: 0.7515\n",
      "Epoch 109/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5087 - acc: 0.7528\n",
      "Epoch 110/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.5072 - acc: 0.7522\n",
      "Epoch 111/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5011 - acc: 0.7576\n",
      "Epoch 112/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.4992 - acc: 0.7598\n",
      "Epoch 113/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5058 - acc: 0.7559\n",
      "Epoch 114/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.4986 - acc: 0.7594\n",
      "Epoch 115/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5004 - acc: 0.7612\n",
      "Epoch 116/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.4987 - acc: 0.7581\n",
      "Epoch 117/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5140 - acc: 0.7494\n",
      "Epoch 118/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.5012 - acc: 0.7560\n",
      "Epoch 119/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.4927 - acc: 0.7631\n",
      "Epoch 120/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.4996 - acc: 0.7589\n",
      "Epoch 121/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.4991 - acc: 0.7601\n",
      "Epoch 122/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.4968 - acc: 0.7643\n",
      "Epoch 123/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.4968 - acc: 0.7573\n",
      "Epoch 124/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.4952 - acc: 0.7622\n",
      "Epoch 125/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5019 - acc: 0.7555\n",
      "Epoch 126/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.4945 - acc: 0.7652\n",
      "Epoch 127/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.4907 - acc: 0.7689\n",
      "Epoch 128/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5009 - acc: 0.7592\n",
      "Epoch 129/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.4965 - acc: 0.7604\n",
      "Epoch 130/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.5143 - acc: 0.7482\n",
      "Epoch 131/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.5105 - acc: 0.7522\n",
      "Epoch 132/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.4938 - acc: 0.7604\n",
      "Epoch 133/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.4979 - acc: 0.7612\n",
      "Epoch 134/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.4991 - acc: 0.7591\n",
      "Epoch 135/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.4951 - acc: 0.7624\n",
      "Epoch 136/150\n",
      "16200/16200 [==============================] - 3s 194us/step - loss: 0.5074 - acc: 0.7512\n",
      "Epoch 137/150\n",
      "16200/16200 [==============================] - 3s 191us/step - loss: 0.4938 - acc: 0.7623\n",
      "Epoch 138/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.5029 - acc: 0.7544\n",
      "Epoch 139/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.5115 - acc: 0.7465\n",
      "Epoch 140/150\n",
      "16200/16200 [==============================] - 3s 177us/step - loss: 0.5027 - acc: 0.7570\n",
      "Epoch 141/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.4949 - acc: 0.7627 1\n",
      "Epoch 142/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.4993 - acc: 0.7586\n",
      "Epoch 143/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.4961 - acc: 0.7591\n",
      "Epoch 144/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.4892 - acc: 0.7625\n",
      "Epoch 145/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.5016 - acc: 0.7567\n",
      "Epoch 146/150\n",
      "16200/16200 [==============================] - 3s 185us/step - loss: 0.4972 - acc: 0.7596\n",
      "Epoch 147/150\n",
      "16200/16200 [==============================] - 3s 188us/step - loss: 0.4974 - acc: 0.7578\n",
      "Epoch 148/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.4942 - acc: 0.7619\n",
      "Epoch 149/150\n",
      "16200/16200 [==============================] - 3s 191us/step - loss: 0.4969 - acc: 0.7611\n",
      "Epoch 150/150\n",
      "16200/16200 [==============================] - 3s 190us/step - loss: 0.5141 - acc: 0.7470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18e658bd470>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2\n",
    "# Fit the model\n",
    "model.fit(dataHistogram(X, 10000), y, epochs=150, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 0s 169us/step\n"
     ]
    }
   ],
   "source": [
    "(loss, accuracy) = model.evaluate(dataHistogram(Xtest, 10000), ytest, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4833895152144962"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7877777777777778"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(dataHistogram(Xtest, 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[674 226]\n",
      " [156 744]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(ytest.argmax(axis=1), predict.argmax(axis=1), [0, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[883  17]\n",
      " [ 71  29]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(ytest.argmax(axis=1), predict.argmax(axis=1), [0, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbey', 'airport_terminal', 'amphitheater', 'amusement_park', 'aquarium', 'aqueduct', 'art_gallery', 'assembly_line', 'auditorium']\n",
      "Loading Topic: abbey\n",
      "Loading Topic: airport_terminal\n",
      "Loading Topic: amphitheater\n",
      "Loading Topic: amusement_park\n",
      "Loading Topic: aquarium\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-41c9f4f6ceb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mloadFilteredData\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#loading all data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0morigImg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadAllTopicData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\cs4476\\Inverse\\loadFilteredData.py\u001b[0m in \u001b[0;36mloadAllTopicData\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0morigImg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mfilImg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0morigImg_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilImg_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadTopicData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0morigImg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigImg_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\cs4476\\Inverse\\loadFilteredData.py\u001b[0m in \u001b[0;36mloadTopicData\u001b[1;34m(topic)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\\\Users\\\\Chunlok Lo\\\\Documents\\\\cs4476\\\\Inverse\\\\data\\\\a\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mfilPath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\\\Users\\\\Chunlok Lo\\\\Documents\\\\cs4476\\\\Inverse\\\\data\\\\a_clarendon\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[0morigImg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilImg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadFilteredData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0morigImg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilImg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\cs4476\\Inverse\\loadFilteredData.py\u001b[0m in \u001b[0;36mloadFilteredData\u001b[1;34m(path, filPath)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mimgList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mimgFil\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabsFilPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mimgFilteredList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgFil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mimgData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   2150\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_dedent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2151\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2152\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   1357\u001b[0m                              \u001b[1;34m'with Pillow installed matplotlib can handle '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m                              'more images' % list(handlers))\n\u001b[1;32m-> 1359\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mpil_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2608\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2609\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2610\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from loadFilteredData import *\n",
    "#loading all data\n",
    "origImg, filimg = loadAllTopicData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbey', 'airport_terminal', 'amphitheater', 'amusement_park', 'aquarium', 'aqueduct', 'art_gallery', 'assembly_line', 'auditorium']\n",
      "Loading Topic1: abbey\n",
      "Loading Topic1: airport_terminal\n",
      "Loading Topic1: amphitheater\n",
      "Loading Topic1: amusement_park\n",
      "Loading Topic1: aquarium\n",
      "Loading Topic1: aqueduct\n",
      "Loading Topic1: art_gallery\n",
      "Loading Topic1: assembly_line\n",
      "Loading Topic1: auditorium\n",
      "(9000, 128, 128, 3)\n",
      "(9000, 128, 128, 3)\n",
      "9000\n",
      "9000\n",
      "[8100. 8100.]\n",
      "[900. 900.]\n"
     ]
    }
   ],
   "source": [
    "from loadFilteredData import *\n",
    "#loading all data\n",
    "origImg, filImg = loadAllTopicData()\n",
    "X, y, Xtest, ytest = createData([origImg, filImg], .9)\n",
    "print(np.sum(y, axis=0))\n",
    "print(np.sum(ytest, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "16200/16200 [==============================] - 4s 255us/step - loss: 8.0165 - acc: 0.4998\n",
      "Epoch 2/150\n",
      "16200/16200 [==============================] - 3s 193us/step - loss: 8.0151 - acc: 0.5000\n",
      "Epoch 3/150\n",
      "16200/16200 [==============================] - 3s 195us/step - loss: 8.0151 - acc: 0.5000\n",
      "Epoch 4/150\n",
      "16200/16200 [==============================] - 3s 202us/step - loss: 8.0151 - acc: 0.5000\n",
      "Epoch 5/150\n",
      "10624/16200 [==================>...........] - ETA: 1s - loss: 7.9548 - acc: 0.5038"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-78c58e507c8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Compile model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataHistogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from preprocessing import *\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=10000, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(dataHistogram(X, 10000), y, epochs=150, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhistogram = dataHistogram(X, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "16200/16200 [==============================] - 3s 208us/step - loss: 6.1833 - acc: 0.5965\n",
      "Epoch 2/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 5.9790 - acc: 0.6117\n",
      "Epoch 3/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 5.4870 - acc: 0.6353\n",
      "Epoch 4/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 5.6521 - acc: 0.6199\n",
      "Epoch 5/150\n",
      "16200/16200 [==============================] - 3s 185us/step - loss: 5.4730 - acc: 0.6322\n",
      "Epoch 6/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 4.5505 - acc: 0.6570\n",
      "Epoch 7/150\n",
      "16200/16200 [==============================] - 3s 186us/step - loss: 1.8320 - acc: 0.6724\n",
      "Epoch 8/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.6960 - acc: 0.6598\n",
      "Epoch 9/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.6258 - acc: 0.6871\n",
      "Epoch 10/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.6025 - acc: 0.7072\n",
      "Epoch 11/150\n",
      "16200/16200 [==============================] - 3s 192us/step - loss: 0.5872 - acc: 0.7155\n",
      "Epoch 12/150\n",
      "16200/16200 [==============================] - 3s 189us/step - loss: 0.5776 - acc: 0.7198\n",
      "Epoch 13/150\n",
      "16200/16200 [==============================] - 3s 186us/step - loss: 0.5819 - acc: 0.7119\n",
      "Epoch 14/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5541 - acc: 0.7309\n",
      "Epoch 15/150\n",
      "16200/16200 [==============================] - 3s 190us/step - loss: 0.5539 - acc: 0.7271\n",
      "Epoch 16/150\n",
      "16200/16200 [==============================] - 3s 192us/step - loss: 0.5552 - acc: 0.7348\n",
      "Epoch 17/150\n",
      "16200/16200 [==============================] - 3s 192us/step - loss: 0.5358 - acc: 0.7399\n",
      "Epoch 18/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.5322 - acc: 0.7424\n",
      "Epoch 19/150\n",
      "16200/16200 [==============================] - 3s 187us/step - loss: 0.5196 - acc: 0.7502\n",
      "Epoch 20/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5150 - acc: 0.7568\n",
      "Epoch 21/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5247 - acc: 0.7481\n",
      "Epoch 22/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.5179 - acc: 0.7503\n",
      "Epoch 23/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5307 - acc: 0.7430\n",
      "Epoch 24/150\n",
      "16200/16200 [==============================] - 3s 187us/step - loss: 0.5072 - acc: 0.7615\n",
      "Epoch 25/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5086 - acc: 0.7579\n",
      "Epoch 26/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.5261 - acc: 0.7424\n",
      "Epoch 27/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.4994 - acc: 0.7637\n",
      "Epoch 28/150\n",
      "16200/16200 [==============================] - 3s 189us/step - loss: 0.5124 - acc: 0.7516\n",
      "Epoch 29/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5176 - acc: 0.7494\n",
      "Epoch 30/150\n",
      "16200/16200 [==============================] - 3s 185us/step - loss: 0.4995 - acc: 0.7638\n",
      "Epoch 31/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5176 - acc: 0.7508\n",
      "Epoch 32/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.4889 - acc: 0.7700\n",
      "Epoch 33/150\n",
      "16200/16200 [==============================] - 3s 185us/step - loss: 0.4946 - acc: 0.7660\n",
      "Epoch 34/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.4954 - acc: 0.7647\n",
      "Epoch 35/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5186 - acc: 0.7547\n",
      "Epoch 36/150\n",
      "16200/16200 [==============================] - 3s 185us/step - loss: 0.5462 - acc: 0.7361\n",
      "Epoch 37/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.5005 - acc: 0.7601\n",
      "Epoch 38/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.4909 - acc: 0.7695\n",
      "Epoch 39/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.4919 - acc: 0.7654\n",
      "Epoch 40/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.4946 - acc: 0.7664\n",
      "Epoch 41/150\n",
      "16200/16200 [==============================] - 3s 190us/step - loss: 0.5013 - acc: 0.7601\n",
      "Epoch 42/150\n",
      "16200/16200 [==============================] - 3s 192us/step - loss: 0.4958 - acc: 0.7610\n",
      "Epoch 43/150\n",
      "16200/16200 [==============================] - 3s 177us/step - loss: 0.5049 - acc: 0.7589\n",
      "Epoch 44/150\n",
      "16200/16200 [==============================] - 3s 200us/step - loss: 0.4794 - acc: 0.7710\n",
      "Epoch 45/150\n",
      "16200/16200 [==============================] - 3s 210us/step - loss: 0.4920 - acc: 0.7651\n",
      "Epoch 46/150\n",
      "16200/16200 [==============================] - 3s 203us/step - loss: 0.4973 - acc: 0.7635 0s - loss: 0.4976 - acc: 0.76\n",
      "Epoch 47/150\n",
      "16200/16200 [==============================] - 3s 192us/step - loss: 0.4907 - acc: 0.7661 1s - loss:\n",
      "Epoch 48/150\n",
      "16200/16200 [==============================] - 3s 187us/step - loss: 0.4698 - acc: 0.7800\n",
      "Epoch 49/150\n",
      "16200/16200 [==============================] - 3s 187us/step - loss: 0.4813 - acc: 0.7743\n",
      "Epoch 50/150\n",
      "16200/16200 [==============================] - 3s 190us/step - loss: 0.5052 - acc: 0.7614\n",
      "Epoch 51/150\n",
      "16200/16200 [==============================] - 3s 189us/step - loss: 0.5064 - acc: 0.7596\n",
      "Epoch 52/150\n",
      "16200/16200 [==============================] - 3s 187us/step - loss: 0.5065 - acc: 0.7511\n",
      "Epoch 53/150\n",
      "16200/16200 [==============================] - 3s 187us/step - loss: 0.4931 - acc: 0.7652\n",
      "Epoch 54/150\n",
      "16200/16200 [==============================] - 3s 187us/step - loss: 0.5006 - acc: 0.7577\n",
      "Epoch 55/150\n",
      "16200/16200 [==============================] - 3s 188us/step - loss: 0.5107 - acc: 0.7499\n",
      "Epoch 56/150\n",
      "16200/16200 [==============================] - 3s 187us/step - loss: 0.5005 - acc: 0.7562\n",
      "Epoch 57/150\n",
      "16200/16200 [==============================] - 3s 187us/step - loss: 0.5041 - acc: 0.7547\n",
      "Epoch 58/150\n",
      "16200/16200 [==============================] - 3s 187us/step - loss: 0.5224 - acc: 0.7622\n",
      "Epoch 59/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5016 - acc: 0.7659\n",
      "Epoch 60/150\n",
      "16200/16200 [==============================] - 3s 186us/step - loss: 0.5063 - acc: 0.7614\n",
      "Epoch 61/150\n",
      "16200/16200 [==============================] - 3s 186us/step - loss: 0.4997 - acc: 0.7586\n",
      "Epoch 62/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.5009 - acc: 0.7636\n",
      "Epoch 63/150\n",
      "16200/16200 [==============================] - 3s 196us/step - loss: 0.4885 - acc: 0.7746 0s - loss: 0\n",
      "Epoch 64/150\n",
      "16200/16200 [==============================] - 3s 189us/step - loss: 0.4831 - acc: 0.7736\n",
      "Epoch 65/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.4893 - acc: 0.7731\n",
      "Epoch 66/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.4818 - acc: 0.7753\n",
      "Epoch 67/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.4776 - acc: 0.7775\n",
      "Epoch 68/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.4745 - acc: 0.7799\n",
      "Epoch 69/150\n",
      "16200/16200 [==============================] - 3s 176us/step - loss: 0.4775 - acc: 0.7770\n",
      "Epoch 70/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.4882 - acc: 0.7698\n",
      "Epoch 71/150\n",
      "16200/16200 [==============================] - 3s 186us/step - loss: 0.4897 - acc: 0.7678\n",
      "Epoch 72/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.4740 - acc: 0.7775\n",
      "Epoch 73/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.4845 - acc: 0.7686\n",
      "Epoch 74/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.4825 - acc: 0.7698\n",
      "Epoch 75/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.4789 - acc: 0.7737\n",
      "Epoch 76/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.4850 - acc: 0.7638\n",
      "Epoch 77/150\n",
      "16200/16200 [==============================] - 3s 186us/step - loss: 0.4743 - acc: 0.7759\n",
      "Epoch 78/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.4812 - acc: 0.7741\n",
      "Epoch 79/150\n",
      "16200/16200 [==============================] - 3s 180us/step - loss: 0.4755 - acc: 0.7752\n",
      "Epoch 80/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.4664 - acc: 0.7844\n",
      "Epoch 81/150\n",
      "16200/16200 [==============================] - 3s 206us/step - loss: 0.4761 - acc: 0.7752\n",
      "Epoch 82/150\n",
      "16200/16200 [==============================] - 3s 186us/step - loss: 0.4923 - acc: 0.7677\n",
      "Epoch 83/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.4669 - acc: 0.7832\n",
      "Epoch 84/150\n",
      "16200/16200 [==============================] - 3s 183us/step - loss: 0.4730 - acc: 0.7810\n",
      "Epoch 85/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.4827 - acc: 0.7702\n",
      "Epoch 86/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.4688 - acc: 0.7839\n",
      "Epoch 87/150\n",
      "16200/16200 [==============================] - 3s 202us/step - loss: 0.4644 - acc: 0.7830\n",
      "Epoch 88/150\n",
      "16200/16200 [==============================] - 3s 196us/step - loss: 0.4770 - acc: 0.7767\n",
      "Epoch 89/150\n",
      "16200/16200 [==============================] - 3s 188us/step - loss: 0.4763 - acc: 0.7781\n",
      "Epoch 90/150\n",
      "16200/16200 [==============================] - 3s 191us/step - loss: 0.4677 - acc: 0.7813\n",
      "Epoch 91/150\n",
      "16200/16200 [==============================] - 3s 198us/step - loss: 0.4891 - acc: 0.7686\n",
      "Epoch 92/150\n",
      "16200/16200 [==============================] - 3s 189us/step - loss: 0.4777 - acc: 0.7742\n",
      "Epoch 93/150\n",
      "16200/16200 [==============================] - 3s 192us/step - loss: 0.4741 - acc: 0.7822\n",
      "Epoch 94/150\n",
      "16200/16200 [==============================] - 3s 188us/step - loss: 0.4632 - acc: 0.7844\n",
      "Epoch 95/150\n",
      "16200/16200 [==============================] - 3s 187us/step - loss: 0.4670 - acc: 0.7815\n",
      "Epoch 96/150\n",
      "16200/16200 [==============================] - 3s 188us/step - loss: 0.4794 - acc: 0.7756\n",
      "Epoch 97/150\n",
      "16200/16200 [==============================] - 3s 185us/step - loss: 0.4723 - acc: 0.7798\n",
      "Epoch 98/150\n",
      "16200/16200 [==============================] - 3s 186us/step - loss: 0.4902 - acc: 0.7676\n",
      "Epoch 99/150\n",
      "16200/16200 [==============================] - 3s 186us/step - loss: 0.4787 - acc: 0.7778\n",
      "Epoch 100/150\n",
      "16200/16200 [==============================] - 3s 189us/step - loss: 0.4745 - acc: 0.7760\n",
      "Epoch 101/150\n",
      "16200/16200 [==============================] - 3s 188us/step - loss: 0.4658 - acc: 0.7846\n",
      "Epoch 102/150\n",
      "16200/16200 [==============================] - 3s 189us/step - loss: 0.4622 - acc: 0.7865\n",
      "Epoch 103/150\n",
      "16200/16200 [==============================] - 3s 185us/step - loss: 0.4669 - acc: 0.7832\n",
      "Epoch 104/150\n",
      "16200/16200 [==============================] - 3s 192us/step - loss: 0.4711 - acc: 0.7785\n",
      "Epoch 105/150\n",
      "16200/16200 [==============================] - 3s 191us/step - loss: 0.4648 - acc: 0.7863\n",
      "Epoch 106/150\n",
      "16200/16200 [==============================] - 3s 188us/step - loss: 0.4664 - acc: 0.7844\n",
      "Epoch 107/150\n",
      "16200/16200 [==============================] - 3s 187us/step - loss: 0.4649 - acc: 0.7867\n",
      "Epoch 108/150\n",
      "16200/16200 [==============================] - 3s 189us/step - loss: 0.4739 - acc: 0.7803\n",
      "Epoch 109/150\n",
      "16200/16200 [==============================] - 3s 189us/step - loss: 0.4685 - acc: 0.7820\n",
      "Epoch 110/150\n",
      "16200/16200 [==============================] - 3s 194us/step - loss: 0.4661 - acc: 0.7867\n",
      "Epoch 111/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.4772 - acc: 0.7772\n",
      "Epoch 112/150\n",
      "16200/16200 [==============================] - 3s 185us/step - loss: 0.4753 - acc: 0.7773\n",
      "Epoch 113/150\n",
      "16200/16200 [==============================] - 3s 181us/step - loss: 0.4717 - acc: 0.7789\n",
      "Epoch 114/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.4705 - acc: 0.7786\n",
      "Epoch 115/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.4676 - acc: 0.7783\n",
      "Epoch 116/150\n",
      "16200/16200 [==============================] - 3s 184us/step - loss: 0.4665 - acc: 0.7817\n",
      "Epoch 117/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.4649 - acc: 0.7846\n",
      "Epoch 118/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.4736 - acc: 0.7765\n",
      "Epoch 119/150\n",
      "16200/16200 [==============================] - 3s 209us/step - loss: 0.4697 - acc: 0.7802\n",
      "Epoch 120/150\n",
      "16200/16200 [==============================] - 3s 199us/step - loss: 0.4869 - acc: 0.7682\n",
      "Epoch 121/150\n",
      "16200/16200 [==============================] - 3s 190us/step - loss: 0.4698 - acc: 0.7824\n",
      "Epoch 122/150\n",
      "16200/16200 [==============================] - 3s 191us/step - loss: 0.4653 - acc: 0.7824\n",
      "Epoch 123/150\n",
      "16200/16200 [==============================] - 3s 189us/step - loss: 0.4639 - acc: 0.7833\n",
      "Epoch 124/150\n",
      "16200/16200 [==============================] - 3s 197us/step - loss: 0.4644 - acc: 0.7858 1s - los\n",
      "Epoch 125/150\n",
      "16200/16200 [==============================] - 3s 190us/step - loss: 0.4574 - acc: 0.7880\n",
      "Epoch 126/150\n",
      "16200/16200 [==============================] - 3s 197us/step - loss: 0.4609 - acc: 0.7860\n",
      "Epoch 127/150\n",
      "16200/16200 [==============================] - 3s 196us/step - loss: 0.4596 - acc: 0.7835\n",
      "Epoch 128/150\n",
      "16200/16200 [==============================] - 3s 200us/step - loss: 0.4752 - acc: 0.7741\n",
      "Epoch 129/150\n",
      "16200/16200 [==============================] - 3s 192us/step - loss: 0.4733 - acc: 0.7767\n",
      "Epoch 130/150\n",
      "16200/16200 [==============================] - 3s 192us/step - loss: 0.4771 - acc: 0.7737\n",
      "Epoch 131/150\n",
      "16200/16200 [==============================] - 3s 196us/step - loss: 0.4635 - acc: 0.7859\n",
      "Epoch 132/150\n",
      "16200/16200 [==============================] - 3s 186us/step - loss: 0.4608 - acc: 0.7856\n",
      "Epoch 133/150\n",
      "16200/16200 [==============================] - 3s 187us/step - loss: 0.4600 - acc: 0.7848\n",
      "Epoch 134/150\n",
      "16200/16200 [==============================] - 3s 198us/step - loss: 0.4675 - acc: 0.7837\n",
      "Epoch 135/150\n",
      "16200/16200 [==============================] - 3s 203us/step - loss: 0.4746 - acc: 0.7752\n",
      "Epoch 136/150\n",
      "16200/16200 [==============================] - 3s 205us/step - loss: 0.4658 - acc: 0.7823\n",
      "Epoch 137/150\n",
      "16200/16200 [==============================] - 3s 204us/step - loss: 0.4605 - acc: 0.7877\n",
      "Epoch 138/150\n",
      "16200/16200 [==============================] - 3s 208us/step - loss: 0.4682 - acc: 0.7800\n",
      "Epoch 139/150\n",
      "16200/16200 [==============================] - 3s 204us/step - loss: 0.4612 - acc: 0.7854\n",
      "Epoch 140/150\n",
      "16200/16200 [==============================] - 3s 206us/step - loss: 0.4585 - acc: 0.7902\n",
      "Epoch 141/150\n",
      "16200/16200 [==============================] - 3s 206us/step - loss: 0.4749 - acc: 0.7741\n",
      "Epoch 142/150\n",
      "16200/16200 [==============================] - 3s 203us/step - loss: 0.4593 - acc: 0.7876\n",
      "Epoch 143/150\n",
      "16200/16200 [==============================] - 3s 203us/step - loss: 0.4551 - acc: 0.7902\n",
      "Epoch 144/150\n",
      "16200/16200 [==============================] - 3s 203us/step - loss: 0.4598 - acc: 0.7861\n",
      "Epoch 145/150\n",
      "16200/16200 [==============================] - 3s 190us/step - loss: 0.4682 - acc: 0.7814\n",
      "Epoch 146/150\n",
      "16200/16200 [==============================] - 3s 191us/step - loss: 0.4584 - acc: 0.7879\n",
      "Epoch 147/150\n",
      "16200/16200 [==============================] - 3s 196us/step - loss: 0.4589 - acc: 0.7875\n",
      "Epoch 148/150\n",
      "16200/16200 [==============================] - 3s 192us/step - loss: 0.4631 - acc: 0.7856\n",
      "Epoch 149/150\n",
      "16200/16200 [==============================] - 3s 188us/step - loss: 0.4632 - acc: 0.7841\n",
      "Epoch 150/150\n",
      "16200/16200 [==============================] - 3s 192us/step - loss: 0.4685 - acc: 0.7798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27b5b8a7550>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(48, input_dim=10000, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(Xhistogram, y, epochs=150, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "16200/16200 [==============================] - 3s 200us/step - loss: 6.3718 - acc: 0.5894\n",
      "Epoch 2/150\n",
      "16200/16200 [==============================] - 3s 161us/step - loss: 6.1519 - acc: 0.6072\n",
      "Epoch 3/150\n",
      "16200/16200 [==============================] - 3s 159us/step - loss: 5.9599 - acc: 0.6181\n",
      "Epoch 4/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 5.8574 - acc: 0.6212\n",
      "Epoch 5/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 6.0595 - acc: 0.6133\n",
      "Epoch 6/150\n",
      "16200/16200 [==============================] - 3s 164us/step - loss: 5.8210 - acc: 0.6270 1s - loss: 5.9212 \n",
      "Epoch 7/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 5.8820 - acc: 0.6233\n",
      "Epoch 8/150\n",
      "16200/16200 [==============================] - 3s 164us/step - loss: 5.6960 - acc: 0.6280\n",
      "Epoch 9/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 5.4733 - acc: 0.6417\n",
      "Epoch 10/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 5.2744 - acc: 0.6407\n",
      "Epoch 11/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 3.9569 - acc: 0.6384\n",
      "Epoch 12/150\n",
      "16200/16200 [==============================] - 3s 166us/step - loss: 3.1309 - acc: 0.6482\n",
      "Epoch 13/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 1.3090 - acc: 0.6575\n",
      "Epoch 14/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 0.7759 - acc: 0.6750\n",
      "Epoch 15/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 0.6026 - acc: 0.7260\n",
      "Epoch 16/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.5985 - acc: 0.7180\n",
      "Epoch 17/150\n",
      "16200/16200 [==============================] - 3s 164us/step - loss: 0.5755 - acc: 0.7288\n",
      "Epoch 18/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.6238 - acc: 0.6975\n",
      "Epoch 19/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 0.5673 - acc: 0.7262\n",
      "Epoch 20/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 0.5492 - acc: 0.7367\n",
      "Epoch 21/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 0.5498 - acc: 0.7377\n",
      "Epoch 22/150\n",
      "16200/16200 [==============================] - 3s 165us/step - loss: 0.5453 - acc: 0.7392\n",
      "Epoch 23/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 0.5294 - acc: 0.7486\n",
      "Epoch 24/150\n",
      "16200/16200 [==============================] - 3s 170us/step - loss: 0.5348 - acc: 0.7454\n",
      "Epoch 25/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.5314 - acc: 0.7425\n",
      "Epoch 26/150\n",
      "16200/16200 [==============================] - 3s 182us/step - loss: 0.5517 - acc: 0.7347 0s - loss: 0.5537 - acc: 0\n",
      "Epoch 27/150\n",
      "16200/16200 [==============================] - 3s 171us/step - loss: 0.5193 - acc: 0.7552\n",
      "Epoch 28/150\n",
      "16200/16200 [==============================] - 3s 172us/step - loss: 0.5352 - acc: 0.7443\n",
      "Epoch 29/150\n",
      "16200/16200 [==============================] - 3s 164us/step - loss: 0.5191 - acc: 0.7562\n",
      "Epoch 30/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 0.5082 - acc: 0.7625\n",
      "Epoch 31/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.5036 - acc: 0.7648\n",
      "Epoch 32/150\n",
      "16200/16200 [==============================] - 3s 161us/step - loss: 0.5120 - acc: 0.7599\n",
      "Epoch 33/150\n",
      "16200/16200 [==============================] - 3s 164us/step - loss: 0.5276 - acc: 0.7505\n",
      "Epoch 34/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.5044 - acc: 0.7646\n",
      "Epoch 35/150\n",
      "16200/16200 [==============================] - 3s 164us/step - loss: 0.5013 - acc: 0.7615\n",
      "Epoch 36/150\n",
      "16200/16200 [==============================] - 3s 161us/step - loss: 0.4872 - acc: 0.7733\n",
      "Epoch 37/150\n",
      "16200/16200 [==============================] - 3s 164us/step - loss: 0.4889 - acc: 0.7702 2\n",
      "Epoch 38/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.4869 - acc: 0.7748\n",
      "Epoch 39/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.5044 - acc: 0.7647\n",
      "Epoch 40/150\n",
      "16200/16200 [==============================] - 3s 164us/step - loss: 0.5042 - acc: 0.7608\n",
      "Epoch 41/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.4732 - acc: 0.7806\n",
      "Epoch 42/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.4993 - acc: 0.7653\n",
      "Epoch 43/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.4951 - acc: 0.7680\n",
      "Epoch 44/150\n",
      "16200/16200 [==============================] - 3s 165us/step - loss: 0.4763 - acc: 0.7801\n",
      "Epoch 45/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.4819 - acc: 0.7741\n",
      "Epoch 46/150\n",
      "16200/16200 [==============================] - 3s 166us/step - loss: 0.4904 - acc: 0.7671\n",
      "Epoch 47/150\n",
      "16200/16200 [==============================] - 3s 167us/step - loss: 0.4770 - acc: 0.7751\n",
      "Epoch 48/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.4708 - acc: 0.7814\n",
      "Epoch 49/150\n",
      "16200/16200 [==============================] - 3s 167us/step - loss: 0.4743 - acc: 0.7774\n",
      "Epoch 50/150\n",
      "16200/16200 [==============================] - 3s 167us/step - loss: 0.4763 - acc: 0.7827\n",
      "Epoch 51/150\n",
      "16200/16200 [==============================] - 3s 179us/step - loss: 0.4846 - acc: 0.7722\n",
      "Epoch 52/150\n",
      "16200/16200 [==============================] - 3s 166us/step - loss: 0.5122 - acc: 0.7547\n",
      "Epoch 53/150\n",
      "16200/16200 [==============================] - 3s 164us/step - loss: 0.4788 - acc: 0.7739\n",
      "Epoch 54/150\n",
      "16200/16200 [==============================] - 3s 165us/step - loss: 0.4866 - acc: 0.7691\n",
      "Epoch 55/150\n",
      "16200/16200 [==============================] - 3s 171us/step - loss: 0.4942 - acc: 0.7650\n",
      "Epoch 56/150\n",
      "16200/16200 [==============================] - 3s 178us/step - loss: 0.4640 - acc: 0.7821\n",
      "Epoch 57/150\n",
      "16200/16200 [==============================] - 3s 186us/step - loss: 0.4541 - acc: 0.7901\n",
      "Epoch 58/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 0.4528 - acc: 0.7872\n",
      "Epoch 59/150\n",
      "16200/16200 [==============================] - 3s 160us/step - loss: 0.4593 - acc: 0.7865\n",
      "Epoch 60/150\n",
      "16200/16200 [==============================] - 3s 158us/step - loss: 0.4564 - acc: 0.7881\n",
      "Epoch 61/150\n",
      "16200/16200 [==============================] - 3s 159us/step - loss: 0.4560 - acc: 0.7874\n",
      "Epoch 62/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.4785 - acc: 0.7764 1s - l\n",
      "Epoch 63/150\n",
      "16200/16200 [==============================] - 3s 160us/step - loss: 0.4691 - acc: 0.7769\n",
      "Epoch 64/150\n",
      "16200/16200 [==============================] - 3s 159us/step - loss: 0.4502 - acc: 0.7897\n",
      "Epoch 65/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.4542 - acc: 0.7913\n",
      "Epoch 66/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.4648 - acc: 0.7793\n",
      "Epoch 67/150\n",
      "16200/16200 [==============================] - 3s 161us/step - loss: 0.4565 - acc: 0.7876\n",
      "Epoch 68/150\n",
      "16200/16200 [==============================] - 3s 161us/step - loss: 0.4520 - acc: 0.7891\n",
      "Epoch 69/150\n",
      "16200/16200 [==============================] - 3s 159us/step - loss: 0.4521 - acc: 0.7880\n",
      "Epoch 70/150\n",
      "16200/16200 [==============================] - 3s 161us/step - loss: 0.4506 - acc: 0.7911\n",
      "Epoch 71/150\n",
      "16200/16200 [==============================] - 3s 169us/step - loss: 0.4545 - acc: 0.7907\n",
      "Epoch 72/150\n",
      "16200/16200 [==============================] - 3s 165us/step - loss: 0.4607 - acc: 0.7888\n",
      "Epoch 73/150\n",
      "16200/16200 [==============================] - 3s 161us/step - loss: 0.4496 - acc: 0.7972\n",
      "Epoch 74/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.4396 - acc: 0.7991\n",
      "Epoch 75/150\n",
      "16200/16200 [==============================] - 3s 164us/step - loss: 0.4744 - acc: 0.7790\n",
      "Epoch 76/150\n",
      "16200/16200 [==============================] - 3s 161us/step - loss: 0.4509 - acc: 0.7932\n",
      "Epoch 77/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 0.4362 - acc: 0.7980\n",
      "Epoch 78/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 0.4629 - acc: 0.7851\n",
      "Epoch 79/150\n",
      "16200/16200 [==============================] - 3s 170us/step - loss: 0.4494 - acc: 0.7910\n",
      "Epoch 80/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16200/16200 [==============================] - 3s 164us/step - loss: 0.4523 - acc: 0.7890\n",
      "Epoch 81/150\n",
      "16200/16200 [==============================] - 3s 165us/step - loss: 0.4298 - acc: 0.8017\n",
      "Epoch 82/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 0.4486 - acc: 0.7892\n",
      "Epoch 83/150\n",
      "16200/16200 [==============================] - 3s 165us/step - loss: 0.4366 - acc: 0.7986\n",
      "Epoch 84/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.4359 - acc: 0.7998\n",
      "Epoch 85/150\n",
      "16200/16200 [==============================] - 3s 167us/step - loss: 0.4574 - acc: 0.7881\n",
      "Epoch 86/150\n",
      "16200/16200 [==============================] - 3s 160us/step - loss: 0.4243 - acc: 0.8067\n",
      "Epoch 87/150\n",
      "16200/16200 [==============================] - 3s 165us/step - loss: 0.4367 - acc: 0.7987\n",
      "Epoch 88/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 0.4340 - acc: 0.7975\n",
      "Epoch 89/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.4554 - acc: 0.7888\n",
      "Epoch 90/150\n",
      "16200/16200 [==============================] - 3s 165us/step - loss: 0.4384 - acc: 0.8004\n",
      "Epoch 91/150\n",
      "16200/16200 [==============================] - 3s 168us/step - loss: 0.4472 - acc: 0.7917\n",
      "Epoch 92/150\n",
      "16200/16200 [==============================] - 3s 165us/step - loss: 0.4507 - acc: 0.7910\n",
      "Epoch 93/150\n",
      "16200/16200 [==============================] - 3s 164us/step - loss: 0.4458 - acc: 0.7918\n",
      "Epoch 94/150\n",
      "16200/16200 [==============================] - 3s 163us/step - loss: 0.4260 - acc: 0.8041\n",
      "Epoch 95/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 0.4383 - acc: 0.7977\n",
      "Epoch 96/150\n",
      "16200/16200 [==============================] - 3s 167us/step - loss: 0.4528 - acc: 0.7914\n",
      "Epoch 97/150\n",
      "16200/16200 [==============================] - 3s 162us/step - loss: 0.4389 - acc: 0.7954\n",
      "Epoch 98/150\n",
      "16200/16200 [==============================] - 3s 167us/step - loss: 0.4274 - acc: 0.8026\n",
      "Epoch 99/150\n",
      "16200/16200 [==============================] - 3s 173us/step - loss: 0.4202 - acc: 0.8056\n",
      "Epoch 100/150\n",
      "16200/16200 [==============================] - 3s 171us/step - loss: 0.4338 - acc: 0.8016\n",
      "Epoch 101/150\n",
      "16200/16200 [==============================] - 3s 164us/step - loss: 0.4332 - acc: 0.7975\n",
      "Epoch 102/150\n",
      "16200/16200 [==============================] - 3s 168us/step - loss: 0.4478 - acc: 0.7894\n",
      "Epoch 103/150\n",
      "16200/16200 [==============================] - 3s 165us/step - loss: 0.4373 - acc: 0.7973\n",
      "Epoch 104/150\n",
      "16200/16200 [==============================] - 3s 170us/step - loss: 0.4371 - acc: 0.7954\n",
      "Epoch 105/150\n",
      "16200/16200 [==============================] - 3s 165us/step - loss: 0.4258 - acc: 0.8054\n",
      "Epoch 106/150\n",
      "16200/16200 [==============================] - 3s 166us/step - loss: 0.4175 - acc: 0.8101\n",
      "Epoch 107/150\n",
      "16200/16200 [==============================] - 3s 161us/step - loss: 0.4358 - acc: 0.7987\n",
      "Epoch 108/150\n",
      "16200/16200 [==============================] - 3s 164us/step - loss: 0.4401 - acc: 0.7951\n",
      "Epoch 109/150\n",
      "16200/16200 [==============================] - ETA: 0s - loss: 0.4436 - acc: 0.7958- ETA: 0s - loss: 0.4450 - acc: 0. - 3s 165us/step - loss: 0.4434 - acc: 0.7959\n",
      "Epoch 110/150\n",
      "16200/16200 [==============================] - 3s 168us/step - loss: 0.4291 - acc: 0.8038\n",
      "Epoch 111/150\n",
      "16200/16200 [==============================] - 3s 174us/step - loss: 0.4385 - acc: 0.7957\n",
      "Epoch 112/150\n",
      "16200/16200 [==============================] - 3s 171us/step - loss: 0.4576 - acc: 0.7844\n",
      "Epoch 113/150\n",
      "16200/16200 [==============================] - 3s 172us/step - loss: 0.4312 - acc: 0.8016\n",
      "Epoch 114/150\n",
      "16200/16200 [==============================] - 3s 166us/step - loss: 0.4291 - acc: 0.7995\n",
      "Epoch 115/150\n",
      "16200/16200 [==============================] - 3s 164us/step - loss: 0.4276 - acc: 0.8024\n",
      "Epoch 116/150\n",
      "16200/16200 [==============================] - 4s 234us/step - loss: 0.4395 - acc: 0.7956\n",
      "Epoch 117/150\n",
      "16200/16200 [==============================] - 4s 274us/step - loss: 0.4174 - acc: 0.8096\n",
      "Epoch 118/150\n",
      "16200/16200 [==============================] - 4s 275us/step - loss: 0.4457 - acc: 0.7954\n",
      "Epoch 119/150\n",
      "16200/16200 [==============================] - 5s 284us/step - loss: 0.4297 - acc: 0.8023\n",
      "Epoch 120/150\n",
      "16200/16200 [==============================] - 5s 289us/step - loss: 0.4592 - acc: 0.7853\n",
      "Epoch 121/150\n",
      "16200/16200 [==============================] - 5s 279us/step - loss: 0.4310 - acc: 0.7984\n",
      "Epoch 122/150\n",
      "16200/16200 [==============================] - 4s 276us/step - loss: 0.4295 - acc: 0.8033\n",
      "Epoch 123/150\n",
      "16200/16200 [==============================] - 5s 282us/step - loss: 0.4297 - acc: 0.8030\n",
      "Epoch 124/150\n",
      "16200/16200 [==============================] - 5s 285us/step - loss: 0.4218 - acc: 0.8084\n",
      "Epoch 125/150\n",
      "16200/16200 [==============================] - 5s 281us/step - loss: 0.4185 - acc: 0.8052\n",
      "Epoch 126/150\n",
      "16200/16200 [==============================] - 5s 285us/step - loss: 0.4215 - acc: 0.8047\n",
      "Epoch 127/150\n",
      "16200/16200 [==============================] - 4s 276us/step - loss: 0.4345 - acc: 0.7972\n",
      "Epoch 128/150\n",
      "16200/16200 [==============================] - 5s 281us/step - loss: 0.4168 - acc: 0.8088\n",
      "Epoch 129/150\n",
      "16200/16200 [==============================] - 5s 284us/step - loss: 0.4173 - acc: 0.8070\n",
      "Epoch 130/150\n",
      "16200/16200 [==============================] - 5s 281us/step - loss: 0.4344 - acc: 0.7995\n",
      "Epoch 131/150\n",
      "16200/16200 [==============================] - ETA: 0s - loss: 0.4419 - acc: 0.797 - 5s 278us/step - loss: 0.4418 - acc: 0.7978\n",
      "Epoch 132/150\n",
      "16200/16200 [==============================] - 4s 276us/step - loss: 0.4283 - acc: 0.8026\n",
      "Epoch 133/150\n",
      "16200/16200 [==============================] - 4s 275us/step - loss: 0.4311 - acc: 0.8017\n",
      "Epoch 134/150\n",
      "16200/16200 [==============================] - 4s 275us/step - loss: 0.4362 - acc: 0.7980\n",
      "Epoch 135/150\n",
      "16200/16200 [==============================] - 4s 274us/step - loss: 0.4278 - acc: 0.7998\n",
      "Epoch 136/150\n",
      "16200/16200 [==============================] - 5s 278us/step - loss: 0.4153 - acc: 0.8095\n",
      "Epoch 137/150\n",
      "16200/16200 [==============================] - 5s 278us/step - loss: 0.4149 - acc: 0.8096\n",
      "Epoch 138/150\n",
      "16200/16200 [==============================] - 4s 274us/step - loss: 0.4164 - acc: 0.8064\n",
      "Epoch 139/150\n",
      "16200/16200 [==============================] - 4s 274us/step - loss: 0.4192 - acc: 0.8100\n",
      "Epoch 140/150\n",
      "16200/16200 [==============================] - 4s 276us/step - loss: 0.4107 - acc: 0.8109\n",
      "Epoch 141/150\n",
      "16200/16200 [==============================] - 5s 278us/step - loss: 0.4195 - acc: 0.8082\n",
      "Epoch 142/150\n",
      "16200/16200 [==============================] - 5s 282us/step - loss: 0.4312 - acc: 0.7994\n",
      "Epoch 143/150\n",
      "16200/16200 [==============================] - 5s 286us/step - loss: 0.4117 - acc: 0.8112\n",
      "Epoch 144/150\n",
      "16200/16200 [==============================] - 5s 283us/step - loss: 0.4088 - acc: 0.8134\n",
      "Epoch 145/150\n",
      "16200/16200 [==============================] - 5s 281us/step - loss: 0.4217 - acc: 0.8083\n",
      "Epoch 146/150\n",
      "16200/16200 [==============================] - 4s 278us/step - loss: 0.4205 - acc: 0.8086\n",
      "Epoch 147/150\n",
      "16200/16200 [==============================] - 5s 287us/step - loss: 0.4246 - acc: 0.8048\n",
      "Epoch 148/150\n",
      "16200/16200 [==============================] - 5s 281us/step - loss: 0.4348 - acc: 0.7952\n",
      "Epoch 149/150\n",
      "16200/16200 [==============================] - 5s 281us/step - loss: 0.4195 - acc: 0.8046\n",
      "Epoch 150/150\n",
      "16200/16200 [==============================] - 4s 277us/step - loss: 0.4196 - acc: 0.8097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27b5e6473c8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=10000, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(Xhistogram, y, epochs=150, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(dataHistogram(Xtest, 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[703 197]\n",
      " [171 729]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(ytest.argmax(axis=1), predict.argmax(axis=1), [0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
